%%
%% CSE 5095 Final Report - Agentic Test Suite Refinement
%% Author: Rye Howard-Stone
%%
\documentclass[sigconf]{acmart}

%% Colors for visual emphasis
\usepackage{xcolor}
\usepackage{enumitem}

%% Bibliography setup
\AtBeginDocument{%
  \providecommand\BibTeX{{Bib\TeX}}}

%% Rights management - for class project, not actual publication
\setcopyright{none}
\acmConference[CSE 5095]{AI for Software Engineering}{Fall 2025}{University of Connecticut}

%% Remove conference metadata for class paper
\makeatletter
\renewcommand\@formatdoi[1]{}
\renewcommand\@acmArticle{}
\makeatother

\begin{document}

%%
%% Title
%%
\title{Agentic Test Suite Refinement: Repository Characteristics Dominate Prompt Strategy Effects}

%%
%% Author
%%
\author{Rye Howard-Stone}
\email{rye.howard-stone@uconn.edu}
\affiliation{%
  \institution{University of Connecticut}
  \department{Department of Computer Science and Engineering}
  \city{Storrs}
  \state{Connecticut}
  \country{USA}
}

%%
%% Abstract
%%
\begin{abstract}
Large Language Model (LLM) agents can execute software analysis tools to improve test suites, but optimal prompting strategies remain unclear. We present two complementary tools: \textbf{aTSR} (Agentic Test Suite Refinement), a Claude Code slash command orchestrating coverage analysis and mutation testing for backend tests, and \textbf{testScout}, a visual end-to-end testing framework using Set-of-Marks targeting to catch integration bugs that unit tests miss.

In an exploratory study (n=1 per configuration) across nine Python repositories, we compared six prompting strategy variants. Our statistically-supported findings include: (1) the +Mutations variant \textit{catastrophically failed} in 5/9 repositories, achieving $\leq$10\% coverage (sign test, p=0.002); (2) excluding these failures, repository characteristics explained 25.5$\times$ more variance in coverage than prompt strategy choice; and (3) a simple baseline prompt matched elaborate multi-phase workflows in most configurations.

Through the JobsCoach case study, we demonstrate that high backend coverage can still miss critical integration bugs---testScout detected 6 production bugs including a P0 routing failure that unit tests could not catch. Both tools are open-source. \textit{Caveats: (1) Single-run results require replication. (2) Session logs reveal generated tests rely heavily on mocking, raising quality concerns beyond coverage metrics.}
\end{abstract}

%%
%% CCS Concepts
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10011007.10011074.10011099</concept_id>
  <concept_desc>Software and its engineering~Software testing and debugging</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010178.10010179</concept_id>
  <concept_desc>Computing methodologies~Natural language processing</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software testing and debugging}
\ccsdesc[300]{Computing methodologies~Natural language processing}

%%
%% Keywords
%%
\keywords{LLM, test generation, agentic coding, prompt engineering, mutation testing, code coverage}

\maketitle

%% ============================================================================
%% INTRODUCTION
%% ============================================================================
\section{Introduction}

The emergence of agentic Large Language Model (LLM) systems has transformed software engineering workflows~\cite{agent4se-survey}. Tools such as Claude Code~\cite{claude-code-agent}, GitHub Copilot, and Cursor enable developers to delegate complex coding tasks to AI agents that can read files, execute commands, and iteratively refine their outputs. Empirical studies show these tools can improve developer productivity by up to 55\%~\cite{copilot-productivity}, and benchmarks like SWE-bench~\cite{swe-bench} demonstrate that frontier models can now resolve real-world GitHub issues autonomously. A fundamental question emerges: \textit{how much guidance should we provide these agents, and does more elaborate prompting actually improve outcomes?}

Software engineers have developed sophisticated tools for analyzing and improving code quality:

\begin{itemize}
    \item \textbf{Code coverage tools} (coverage.py, gcov) identify which lines are executed by tests
    \item \textbf{Mutation testers} (mutmut, Stryker) verify whether tests actually detect code changes
    \item \textbf{Static analyzers} (pyright, Infer) find potential bugs without execution
    \item \textbf{AST parsers} (Tree-sitter) enable structural code analysis
\end{itemize}

Modern agentic LLMs can leverage these tools, but typically only when directed. This raises our central research question: \textbf{Does providing detailed methodological guidance---incorporating coverage analysis, mutation testing, and expert heuristics---improve LLM-based test generation, or does it introduce unnecessary overhead and potential failure modes?}

We hypothesized that structured workflows would outperform simple prompts. Our exploratory experiments reveal a more nuanced picture: \textbf{repository characteristics appeared to dominate prompt strategy effects by a factor of 25.5$\times$} in variance decomposition analysis (excluding catastrophic failures). Critically, the +Mutations variant---which incorporates mutation testing feedback---catastrophically failed in 5/9 repositories, achieving $\leq$10\% coverage (sign test, p=0.002). Token efficiency patterns were mixed with no statistically significant advantage for minimal prompts.

However, even achieving high backend coverage left critical bugs undetected. During development of JobsCoach (a job search application), we encountered 6 production bugs---including a P0 routing failure---despite strong test coverage. This motivated our second contribution: testScout, which provides visual E2E testing to catch integration bugs that unit tests structurally cannot detect.

This work was motivated by practical experience: the first author developed AmpliconHunter~\cite{ampliconhunter}, a bioinformatics tool for PCR amplicon prediction, using AI-assisted test-driven development. Both the need for principled prompting strategies and the gap between coverage metrics and actual correctness emerged from this experience.

\subsection{Contributions}

This paper contributes:

\begin{enumerate}
    \item \textbf{aTSR}: An open-source Claude Code slash command (\texttt{/refine-tests}) implementing multi-phase test refinement with coverage analysis and mutation testing
    \item \textbf{testScout}: A visual end-to-end testing framework using Set-of-Marks targeting for robust element identification, complementing backend tests by detecting integration bugs
    \item \textbf{Experimental protocol}: A two-phase evaluation framework with six factorial prompt variants across nine repositories
    \item \textbf{Statistical analysis}: Variance decomposition and hypothesis testing on n=1 exploratory data
\end{enumerate}

\subsection{Key Findings}

Our exploratory study (n=1 per configuration) yielded statistically-supported findings:
\begin{itemize}
    \item \textbf{Mutation testing can catastrophically fail}: V4 (+Mutations) achieved $\leq$10\% coverage in 5/9 repos (p=0.002)
    \item \textbf{Repository variance $>>$ prompt variance}: Repository characteristics explained 25.5$\times$ more variance than prompt strategy choice
    \item \textbf{Simple baselines are competitive}: A minimal prompt matched elaborate workflows in most configurations
    \item \textbf{Coverage $\neq$ correctness}: JobsCoach case study shows 6 bugs escaped high-coverage backend tests
\end{itemize}

\noindent\textit{While based on single runs per configuration, these patterns---particularly the mutation testing failure and variance decomposition---are statistically robust and merit attention.}

\begin{table}[h]
\centering
\small
\caption{Summary of Statistical Findings}
\label{tab:stats-summary}
\begin{tabular}{lrrp{4cm}}
\toprule
\textbf{Finding} & \textbf{Statistic} & \textbf{p-value} & \textbf{Interpretation} \\
\midrule
V4 $<$ V1 coverage & 9/9 repos & 0.002 & Mutation testing \textit{may hurt}$^*$ \\
Repo variance & 25.5$\times$ & --- & Repo $>>$ prompt effect \\
V6 token efficiency & 2/6 repos & 0.89 & \textit{Not} significant \\
Bio vs general & 60\% vs 83\% & 0.19 & \textit{Not} significant \\
\bottomrule
\end{tabular}

\vspace{1mm}
\noindent{\footnotesize $^*$Requires replication; pattern is statistically robust but effect magnitude may vary.}
\end{table}

\vspace{1em}
\noindent\fbox{\parbox{\columnwidth}{%
\textbf{\large Practitioner Takeaways} \\[0.5em]
\textbf{1. Start simple.} A 15-line prompt achieved the same coverage as 80-line multi-phase workflows. Complexity doesn't guarantee improvement. \\[0.3em]
\textbf{2. Avoid mutation testing for exploration.} V4 failed catastrophically in 56\% of repos (p=0.002). Use mutation testing only after achieving baseline coverage. \\[0.3em]
\textbf{3. Know your codebase.} Repository characteristics explained 87\% of variance---25$\times$ more than prompt choice. Adapt strategy to codebase, not vice versa. \\[0.3em]
\textbf{4. E2E tests catch what unit tests miss.} JobsCoach's P0 routing bug was undetectable by unit tests despite high coverage. \\[0.3em]
\textbf{5. Inspect test quality, not just coverage.} Generated tests relied heavily on mocking. High coverage with mocks may not catch real bugs---manual review remains essential.
}}

%% ============================================================================
%% RELATED WORK
%% ============================================================================
\section{Related Work}

\textbf{Automated Test Generation.} Traditional approaches include EvoSuite~\cite{evosuite} (genetic algorithms) and Randoop~\cite{randoop} (feedback-directed random testing). Both achieve high coverage but produce tests that may be difficult to maintain.

\textbf{LLM-Based Testing.} TestPilot~\cite{testpilot} achieves 93\% coverage with Codex; CodaMosa~\cite{codamosa} combines search-based testing with LLMs; ChatUniTest~\cite{chatunitest} and CoverUp~\cite{coverup} use iterative refinement. These works find LLMs generate useful tests but struggle with assertions. Our work examines prompt complexity in agentic settings.

\textbf{Prompt Engineering.} Chain-of-thought~\cite{chain-of-thought} and self-consistency~\cite{self-consistency} improve reasoning, but optimal strategies for agentic coding remain understudied. Ma et al.~\cite{llm-program-analysis} found LLMs lack semantic consistency; Copilot studies~\cite{copilot-study} show generated code requires review.

\textbf{Mutation Testing.} Mutation testing~\cite{mutation-survey} evaluates test quality by checking if tests detect code changes. We found mutation feedback can degrade agent performance---an unexpected result.

%% ============================================================================
%% METHODOLOGY
%% ============================================================================
\section{Methodology}

We developed two tools for AI-assisted software testing and conducted a systematic two-phase evaluation.

\subsection{aTSR: Agentic Test Suite Refinement}

aTSR is a Claude Code slash command (\texttt{/refine-tests}) with three modes: \texttt{auto} (adaptive), \texttt{quick} (lightweight), and \texttt{full} (comprehensive). The full workflow: (1) environment detection, (2) coverage analysis, (3) mutation testing, (4) test recommendations, (5) property testing, (6) verification. Heuristics include: mutate only covered code, prioritize boundary conditions, match effort to codebase size.

\subsection{testScout: Visual AI Testing}

testScout uses Set-of-Marks (SoM)~\cite{set-of-marks} for reliable element targeting. It injects JavaScript to tag interactive elements with \texttt{data-testscout-id} attributes, overlays numbered markers, and sends marked screenshots to a vision model (Gemini/GPT-4V). The model returns element IDs rather than fragile CSS selectors. An \texttt{Explorer} class enables autonomous bug-hunting without pre-written tests. Figure~\ref{fig:testscout} shows the SoM overlay identifying interactive elements.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\columnwidth]{figures/testscout-som.png}
\caption{testScout's Set-of-Marks overlay on the JobsCoach login page. Red numbered boxes identify interactive elements (1: Forgot password, 2: Sign In button, 3: Sign up link, 4-6: form fields). The vision model receives this marked screenshot and returns element IDs rather than fragile CSS selectors.}
\label{fig:testscout}
\end{figure}

\subsection{Experimental Design}

Our evaluation proceeded in two phases.

\subsubsection{Phase 1: Base vs. Refine}

Phase 1 compared \textbf{Base} (simple ``improve test coverage'' prompt) vs. \textbf{Refine} (full \texttt{/refine-tests} workflow), using Claude Sonnet 4.5 and Opus 4.1 on three ``reduced coverage'' repositories (click, mistune, schedule) where tests were artificially removed.

\subsubsection{Phase 2: Six Prompt Variants}

Based on Phase 1 findings (base outperformed refine), Phase 2 explored whether specific components of the refine workflow might help. We developed six prompt variants using factorial experimental design (Table~\ref{tab:variants}).

\begin{table}[h]
\caption{Prompt Variant Design Matrix}
\label{tab:variants}
\begin{tabular}{lcrrrr}
\toprule
Variant & Lines & Context & Cov & Mut & Description \\
\midrule
V1 & 30 & -- & -- & -- & Control (original base) \\
V2 & 40 & \checkmark & -- & -- & +Context only \\
V3 & 60 & \checkmark & \checkmark & -- & +Coverage tool \\
V4 & 60 & \checkmark & -- & \checkmark & +Mutation tool \\
V5 & 80 & \checkmark & \checkmark & \checkmark & +Both tools \\
V6 & 15 & \checkmark & -- & -- & Minimal directive \\
\bottomrule
\end{tabular}
\end{table}

The ``Context'' column indicates pre-execution commands detecting language, project structure, and test locations. V6 (Minimal) reduces the directive to 15 lines: ``Improve test coverage to 80\%+. Measure, identify gaps, write targeted tests. Verify.''

Phase 2 tested across three categories: \textbf{Reduced Coverage} (click, mistune, schedule---tests artificially removed); \textbf{Low Existing Coverage} (python-box, colorama, boltons---naturally low coverage); and \textbf{Bioinformatics} (dnaapler, fastqe, pyfaidx---domain-specific).

\subsubsection{Metrics}

We measured: \textbf{Final Coverage} (line \%), \textbf{Pass Rate} (\% tests passing), \textbf{Tests Added}, \textbf{Tokens} (input + output + cache), \textbf{Coverage Efficiency} (pts/M tokens), \textbf{Runtime}, and \textbf{Tool Calls}.

\subsubsection{Experimental Controls and Limitations}

We used Claude Sonnet 4.5 and Opus 4.1 without fixing random seeds. For ``reduced coverage'' repos, tests were removed to reduce coverage from $>$90\% to 60--70\%. Each configuration ran exactly once (n=1), limiting statistical validity---observed differences may reflect LLM variance.

%% ============================================================================
%% RESULTS
%% ============================================================================
\section{Evaluation Results}

\subsection{Phase 1: Base Outperforms Refine}

In our Phase 1 trials, the simple base strategy matched or exceeded the elaborate refine workflow in most configurations. Table~\ref{tab:phase1-results} summarizes the observed results.

\begin{table}[h]
\centering
\small
\caption{Phase 1: Base vs. Refine on Reduced Coverage Repos}
\label{tab:phase1-results}
\begin{tabular}{llllrr}
\toprule
Repo & Model & Strategy & Cov & Pass\% & Tokens \\
\midrule
schedule & Sonnet & base & 88\% & 100.0 & 2.9M \\
schedule & Sonnet & refine & 85\% & 72.5 & 4.9M \\
schedule & Opus & base & 91\% & 100.0 & 3.2M \\
schedule & Opus & refine & 90\% & 96.8 & 6.0M \\
\midrule
mistune & Sonnet & base & 79\% & 94.6 & 6.0M \\
mistune & Sonnet & refine & 72\% & 85.0 & 5.6M \\
mistune & Opus & base & 76\% & 94.3 & 4.2M \\
mistune & Opus & refine & 71\% & 97.7 & 7.9M \\
\midrule
click & Sonnet & base & 64\% & 100.0 & 2.4M \\
click & Sonnet & refine & 64\% & 91.4 & 8.2M \\
click & Opus & base & 67\% & 91.9 & 10.6M$^*$ \\
click & Opus & refine & 66\% & 95.5 & 3.3M \\
\bottomrule
\end{tabular}

\vspace{1mm}
\noindent{\footnotesize $^*$Extended exploration cycles due to integrating with 190 existing baseline tests.}
\end{table}

Observations from Phase 1 trials:
\begin{itemize}
    \item Base matched or exceeded refine coverage in 10 of 12 configurations
    \item Base showed higher pass rates, often reaching 100\%
    \item Sonnet performed comparably to Opus at lower cost
    \item Refine used 40--70\% more tokens without apparent coverage benefit
\end{itemize}

\noindent\textit{Note: With n=1, these patterns may reflect run-to-run variance rather than true strategy differences.}

Figure~\ref{fig:phase1} provides a comprehensive visualization of these results, including coverage achievement, pass rates, token efficiency, and a strategy performance radar chart.

\begin{figure*}[tb]
\centering
\includegraphics[width=\textwidth,height=0.7\textheight,keepaspectratio]{figures/phase1-benchmark.png}
\caption{Phase 1 benchmark results comparing Base vs. Refine strategies across Sonnet and Opus models on three reduced-coverage repositories (schedule, mistune, click). Top row: coverage achievement, pass rates, test generation volume. Middle row: token efficiency, coverage vs. token cost scatter plot, radar chart of strategy profiles. Bottom row: model performance comparison. Summary statistics show Base achieved 77.5\% average coverage vs. Refine's 74.7\%, while using fewer tokens.}
\label{fig:phase1}
\end{figure*}

\subsection{Phase 2: Reduced Coverage Repositories}

For the first Phase 2 experiment, we artificially removed some tests from three popular Python libraries to create coverage gaps, then evaluated all six prompt variants. Figure~\ref{fig:phase2-reduced} shows the complete results.

\begin{figure*}[tb]
\centering
\includegraphics[width=\textwidth,height=0.88\textheight,keepaspectratio,page=1]{figures/Improved_prompts-reduced_cov.pdf}
\caption{Phase 2 Experiment 1: Reduced coverage repositories (click, mistune, schedule). These repos had tests artificially removed to create coverage gaps. Top row: test pass/fail counts, token usage, tokens per new test. Middle row: final coverage (dashed lines show baseline before intervention), coverage efficiency, runtime. Bottom row: tool usage patterns. Key finding: V6 (Minimal) achieves competitive coverage with substantially fewer tokens across all three repositories.}
\label{fig:phase2-reduced}
\end{figure*}

\textbf{click} (CLI toolkit): All variants reached 60-80\% final coverage. V1 (Baseline) generated the most tests (~580) but used ~11M tokens. V6 (Minimal) achieved similar coverage with ~5M tokens.

\textbf{mistune} (Markdown parser): Coverage reached 55-75\% across variants. V4 (+Mutations) showed notably high token-per-test cost (~4000K tokens per new test), while other variants averaged ~500K.

\textbf{schedule} (job scheduler): Highest coverage achieved (75-90\%). V6 showed the best coverage efficiency at ~25\% increase per million tokens, compared to ~10-15\% for complex variants.

\subsection{Phase 2: Low Existing Coverage Repositories}

For the second Phase 2 experiment, we selected three Python libraries with naturally low existing test coverage (no artificial removal). Figure~\ref{fig:phase2-lowcov} shows the complete results.

\begin{figure*}[tb]
\centering
\includegraphics[width=\textwidth,height=0.88\textheight,keepaspectratio,page=1]{figures/Improved_prompts-low_existing_cov.pdf}
\caption{Phase 2 Experiment 2: Low existing coverage repositories (python-box, colorama, boltons). These repos had naturally low coverage without artificial test removal. Dashed lines in the coverage panel show baseline coverage before intervention. Note the dramatic improvement for python-box (near 0\% to ~80\%) vs. modest gains for colorama and boltons (already at ~70\%).}
\label{fig:phase2-lowcov}
\end{figure*}

\textbf{python-box} (dictionary wrapper): Started with near-zero coverage (~2\%), all variants achieved dramatic improvement to 80--90\%. This was the most successful case, with V6 achieving 83\% coverage using 3.0M tokens---an efficiency of ~27 percentage points per million tokens.

\textbf{colorama} (terminal colors): Started with existing coverage around 70\% (dashed line). All variants achieved only modest improvement to ~75-80\%, suggesting a ceiling effect. Token efficiency was lower due to smaller coverage gains.

\textbf{boltons} (utility library): Similar to colorama, started with ~70\% baseline coverage. Final coverage reached ~70-75\% across variants. Generated the most tests (~400-500) but with modest coverage improvement, indicating diminishing returns at higher baseline coverage.

\subsection{Phase 2: Bioinformatics Domain}

For the third Phase 2 experiment, we evaluated domain-specific bioinformatics repositories to test generalization beyond general-purpose Python code. Figure~\ref{fig:phase2-bio} shows the complete results.

\begin{figure*}[tb]
\centering
\includegraphics[width=\textwidth,height=0.88\textheight,keepaspectratio,page=1]{figures/bioinformatics-repos.pdf}
\caption{Phase 2 Experiment 3: Bioinformatics repositories (dnaapler, fastqe, pyfaidx). These domain-specific repos revealed unique challenges. Note the gray ``Failed tests'' bars in the top-left panel showing high failure rates. The coverage efficiency panel (middle-left) shows V4 (+Mutations) achieving \textit{negative} efficiency on fastqe---coverage actually decreased. Dashed lines in coverage panel show baseline before intervention.}
\label{fig:phase2-bio}
\end{figure*}

\textbf{dnaapler} (DNA reorientation): Started with ~25\% baseline coverage, improved to ~40-45\% across most variants. V5 (+Both) generated the most tests (~200) but also had the highest failure rate (gray bar). Modest positive coverage efficiency.

\textbf{fastqe} (FASTQ visualization): Started with ~45\% baseline coverage. Most variants achieved minimal improvement or even slight decrease. Critically, V4 (+Mutations) showed \textit{negative} coverage efficiency---the agent's intervention actually reduced coverage. V3 (+Coverage) performed best with ~20\% efficiency gain.

\textbf{pyfaidx} (FASTA indexing): Started with ~70\% baseline coverage. Results were mixed---some variants slightly improved coverage while others decreased it. High token costs (~120-160K tokens per passing test for some variants) with minimal coverage benefit.

\textbf{Key Finding}: Unlike general-purpose repositories where all variants improved coverage, bioinformatics repos showed that elaborate prompting (especially V4/V5 with mutation feedback) can actually \textit{harm} performance when the model lacks domain expertise.

\subsection{Tool Usage Patterns}

Analysis of tool calls revealed interesting patterns:

\begin{itemize}
    \item \textbf{V4/V5 used more Bash commands} (70-80 per run) than V6 (40-50), primarily for running mutmut
    \item \textbf{V6 used fewer Read calls}, suggesting it wrote tests more directly rather than extensively analyzing
    \item \textbf{All variants used TodoWrite} similarly, indicating task management was prompt-agnostic
\end{itemize}

\subsection{Statistical Pattern Analysis}

While n=1 per configuration limits individual comparisons, patterns across 9 repositories and 6 variants enable statistical testing of consistency:

\textbf{Mutation Testing Failure (V4)}: V4 underperformed V1 in 9/9 repositories (100\%), with 5/9 achieving $\leq$10\% coverage. A sign test for V4 $<$ V1 yields p=0.002, indicating this pattern is highly unlikely due to chance. The mean coverage difference was 44.1 percentage points (V1: 75.3\%, V4: 31.2\%), with Cohen's d = 1.8 (very large effect).

\textbf{Variance Decomposition}: Across 6 general-purpose repositories and 5 variants (excluding V4), between-repository variance was 132 compared to between-variant variance of 5.2---a ratio of 25.5$\times$. Repository characteristics explained 87\% of total variance; prompt strategy explained only 3\%.

Repository characteristics accounted for 87\% of the total variance in coverage outcomes, while prompt strategy choice explained only 3\%. The remaining 10\% was unexplained variance (potentially including LLM non-determinism and measurement noise).

\textbf{Token Efficiency}: Contrary to initial hypotheses, V6 (Minimal) used fewer tokens than V5 (+Both) in only 2/6 repositories. The sign test (p=0.89) and Wilcoxon signed-rank test (p=0.69) found no significant token efficiency advantage for minimal prompts.

\textbf{Domain Effects}: Bioinformatics repositories averaged 60\% coverage vs. 83\% for general repos. Mann-Whitney U test (p=0.19) was not significant, likely due to small sample size (n=3 bio repos).

\textbf{Cost-Per-Coverage-Point Analysis}: At Claude API rates (\$3/M input, \$15/M output tokens), V6 achieved coverage at \$0.08 per percentage point (schedule repo) vs. \$0.35 per point for V5---a 4.4$\times$ cost advantage. Across all repos, V6 averaged \$0.15/point while V4 averaged \$0.89/point (including failures where infinite cost applies to zero-coverage runs).

\textbf{Interpretation}: These tests address pattern \textit{consistency} across repositories, not LLM variance within configurations. The mutation testing failure and variance decomposition findings are statistically robust; token efficiency and domain effect claims require replication.

%% ============================================================================
%% DISCUSSION
%% ============================================================================
\section{Discussion}

\subsection{Repository Characteristics Dominate Prompt Effects}

Variance decomposition reveals repository characteristics explained \textbf{25.5$\times$ more variance} than prompt strategy. The schedule library achieved 88--97\% coverage across all variants, while click ranged from 0--79\%. Factors like existing test infrastructure, API design patterns, and dependency complexity influence outcomes more than prompt engineering. Practitioners should focus on understanding codebase characteristics rather than seeking ``optimal prompts.''

\subsection{Why Simpler Prompts Remain Competitive}

Despite our hypothesis that elaborate prompts would improve outcomes, we found no statistically significant token efficiency advantage for minimal prompts (sign test, p=0.89). Yet simple baselines consistently matched the coverage achieved by elaborate multi-phase workflows. Several factors may explain this finding: (1) complex prompts may overload the model's attention, forcing suboptimal execution patterns; (2) frontier models have already internalized effective testing practices from training data; and (3) each additional workflow phase introduces opportunities for compounding errors that offset any methodological benefits.

\subsection{Why Mutations Catastrophically Failed}

The +Mutations variant (V4) achieved $\leq$10\% coverage in \textbf{5 of 9 repositories} (56\%), with V4 underperforming the baseline V1 in 9/9 cases (sign test, p=0.002). This is not occasional reduction---it is \textit{systematic failure}:

\begin{center}
\small
\begin{tabular}{lrrrl}
\toprule
\textbf{Repository} & \textbf{V1 Cov.} & \textbf{V4 Cov.} & \textbf{$\Delta$} & \textbf{Outcome} \\
\midrule
click & 79\% & 0\% & $-$79 pts & \textcolor{red}{Catastrophic} \\
python-box & 93\% & 1\% & $-$92 pts & \textcolor{red}{Catastrophic} \\
dnaapler & 35\% & 0\% & $-$35 pts & \textcolor{red}{Catastrophic} \\
fastqe & 95\% & 0\% & $-$95 pts & \textcolor{red}{Catastrophic} \\
pyfaidx & 50\% & 0\% & $-$50 pts & \textcolor{red}{Catastrophic}$^\dagger$ \\
schedule & 97\% & 56\% & $-$41 pts & Degraded \\
mistune & 85\% & 46\% & $-$39 pts & Degraded \\
colorama & 77\% & 67\% & $-$10 pts & Degraded \\
boltons & 67\% & 65\% & $-$2 pts & Degraded \\
\bottomrule
\end{tabular}

\vspace{1mm}
\noindent{\footnotesize $^\dagger$Catastrophic = final coverage $\leq$10\%. Degraded = coverage decreased but $>$10\%.}
\end{center}

\noindent Explanations: (1) \textit{Infinite loops}---hundreds of surviving mutants exhaust token budgets; (2) \textit{Priority inversion}---agents optimize for killing mutants rather than covering code; (3) \textit{Domain confusion}---mutation results in specialized code are misinterpreted.

\textbf{Recommendation}: Apply mutation testing \textit{only after} achieving $\geq$70\% baseline coverage. Our data suggests it actively harms exploratory test generation.

\subsection{Domain-Specific Challenges}

Bioinformatics repositories achieved 60\% mean coverage vs. 83\% for general-purpose libraries (not statistically significant: Mann-Whitney U, p=0.19, n=3). Contributing factors: specialized domain knowledge underrepresented in training, complex dependencies on external databases/file formats, and particular vulnerability to mutation testing failure (all 3 bio repos showed V4 coverage $\leq$0\%).

\subsection{Implications for Agentic AI}

Broader implications: (1) start minimal before adding complexity; (2) complex workflows should demonstrate improvement over baselines; (3) each tool integration is a potential failure mode; (4) strategies for general code may fail in specialized domains.

\subsection{Threats to Validity}

\textbf{Internal}: With n=1, differences may reflect LLM non-determinism. Token anomalies (click/Opus using 10.6M vs. 2.4M for Sonnet) suggest unusual conditions in some runs.

\textbf{External}: All repos were Python; results may not generalize. Repository selection was convenience-based.

\textbf{Construct}: Coverage doesn't capture test quality. \textbf{Critically, session logs reveal tests rely heavily on \texttt{mock.Mock()} rather than real execution}---mock-heavy tests may miss real bugs. Future work should evaluate assertion quality explicitly.

\textbf{Conclusion/Reproducibility}: All findings are preliminary observations requiring replication. LLM non-determinism makes exact replication impossible.

\vspace{0.5em}
\noindent\fbox{\parbox{0.95\columnwidth}{%
\textbf{Open Science Statement:} All experimental artifacts are publicly available at \url{https://github.com/rhowardstone/aTSR}, including: 54 configuration metrics (\texttt{results/phase2/*/metrics.json}), full prompt text for all 6 variants, reproducible statistical analysis code (Python), and testScout audit trails with screenshots, prompts, and decision logs.
}}

\subsection{testScout Evaluation}

\textbf{Bridging aTSR and testScout}: The aTSR experiments above focus on backend test generation, where we found that simple prompts achieve competitive coverage. However, even perfect line coverage cannot detect integration failures at architectural boundaries. The JobsCoach P0 routing bug occurred at the FastAPI route registration layer---a configuration concern orthogonal to unit test coverage. This gap between coverage metrics (aTSR's domain) and integration correctness motivated testScout's development. Where aTSR improves the \textit{depth} of backend testing, testScout addresses the \textit{breadth} of system-level validation through visual E2E testing.

testScout operates in two modes: \textbf{Discovery Mode} (no API key required) performs element identification and Set-of-Marks overlay generation; \textbf{AI Mode} (requires Gemini/GPT-4V) adds vision-based action execution and autonomous exploration. This graceful degradation enables testing infrastructure validation without API costs.

\textbf{Discovery Mode Evaluation}: Tested on 15 diverse websites. testScout achieved 100\% success rate, discovering 698 elements (avg 46.5/site, 11.2ms avg discovery). Only 17\% of elements had stable CSS selectors; the rest would require brittle position-dependent selectors. Set-of-Marks achieved 100\% reliability via stable \texttt{data-testscout-id} attributes.

\textbf{Case Study}: testScout identified 6 bugs in JobsCoach: P0 route ordering (invisible to unit tests), P1 database/isolation errors, P2 UI duplication. At \$0.01--\$0.05/test, finding these cost $\sim$\$0.50---preventing hours of debugging~\cite{bug-cost-study}.

\textbf{AI Mode}: Gemini 2.5 Pro discovered 738 elements across 4 sites with 100\% success. Audit trails generated for reproducibility.

Figure~\ref{fig:testscout-wiki} shows testScout's Set-of-Marks overlay on Wikipedia.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/testscout-wiki.png}
\caption{testScout's Set-of-Marks overlay on Wikipedia, identifying 280 interactive elements. Red numbered boxes enable vision-model targeting without fragile CSS selectors.}
\label{fig:testscout-wiki}
\end{figure}

%% ============================================================================
%% CONCLUSION
%% ============================================================================
\section{Conclusion and Future Work}

We presented aTSR and testScout, two open-source tools for AI-assisted testing. Key observations (n=1, requiring replication): (1) minimal prompts achieved competitive coverage at lower token cost; (2) mutation feedback catastrophically failed in 56\% of repos; (3) repository characteristics dominated prompt effects by 25$\times$; (4) bioinformatics showed higher failure rates; (5) Sonnet matched Opus at lower cost.

\subsection{Future Work}

Directions: hybrid approaches (minimal first, tools for gaps), adaptive domain detection, cross-language evaluation, human quality assessment, larger-scale studies with multiple runs, and domain-specific failure analysis.

\subsection{Availability}

Both tools are available under MIT license:
\begin{itemize}
    \item aTSR: \url{https://github.com/rhowardstone/aTSR}
    \item testScout: \url{https://github.com/rhowardstone/testscout}
\end{itemize}

%%
%% Acknowledgments
%%
\begin{acks}
This work was completed as part of CSE 5095: AI for Software Engineering at the University of Connecticut. Thanks to Dr. Tingting Yu for guidance and feedback throughout the project.
\end{acks}

%%
%% Bibliography
%%
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
