# aTSR - Agentic Test Suite Refinement

**Complete test automation for Claude Code**: From zero tests to comprehensive, high-quality test suites.

Created as a project for CSE5095: AI for Software Development (Fall 2025)

---

## ğŸ¯ What is aTSR?

aTSR provides **end-to-end test lifecycle management** through Claude Code skills:

- **Test Creation**: Automated initial test suite generation from source code analysis
- **Test Refinement**: Systematic improvement using coverage analysis and mutation testing
- **Multi-Language**: Python, JavaScript/TypeScript, Java, C/C++ support
- **Scientific Methodology**: Proven approach with benchmarked results

### How We Compare

| Feature | aTSR | obra/superpowers | Baseline |
|---------|------|------------------|----------|
| **Test Creation** | âœ… Batch analysis (398 functions â†’ 50 tests) | âš ï¸ Incremental TDD (test-by-test) | âŒ Ad-hoc |
| **Test Refinement** | âœ… Coverage + Mutation + Properties | âŒ Not available | âŒ Guesswork |
| **Use Case** | Existing codebases needing tests | New feature development | Any |
| **Approach** | Analysis-driven (batch) | Test-first discipline (incremental) | Prompt-based |
| **Multi-Language** | âœ… Python, JS/TS, Java, C/C++ | âš ï¸ Framework agnostic | âš ï¸ Language dependent |
| **Methodology** | âœ… Scientific (measured) | âœ… Systematic (TDD) | âŒ None |
| **Benchmarked** | âœ… Yes | âŒ No | âœ… Yes |

**Complementary to obra/superpowers**:
- **obra TDD**: Best for writing NEW features (test-first, incremental RED-GREEN-REFACTOR)
- **aTSR**: Best for EXISTING codebases (batch analysis, coverage-driven refinement)

---

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/rhowardstone/aTSR.git
cd aTSR
```

### Install Skills

```bash
# For personal use (all projects)
cp -r .claude/skills/* ~/.claude/skills/
cp -r .claude/commands/* ~/.claude/commands/
cp -r .claude/lib ~/.claude/

# Add tools to PATH (add to ~/.bashrc or ~/.zshrc for persistence)
export PATH="$(pwd)/.claude/lib/atsr-tools:$PATH"
```

---

## ğŸ’¡ Usage

```bash
cd your-project
claude

# In Claude Code:
/refine-tests
```

**What happens:**
1. aTSR assesses your codebase size and language
2. Determines if you need test creation or refinement
3. Automatically runs appropriate workflow:
   - **No tests?** â†’ Analyzes code, generates test suite
   - **Some tests?** â†’ Identifies gaps via coverage, improves quality via mutations
   - **Good tests?** â†’ Adds property-based tests for algorithms

---

## ğŸ—ï¸ Architecture

### Skills

aTSR provides 7 specialized skills:

| Skill | Purpose |
|-------|---------|
| **test-suite-management** | Main orchestrator - routes to creation or refinement |
| **test-creation** | Generate initial tests from code analysis (zero â†’ baseline) |
| **test-refinement** | Systematic improvement (baseline â†’ comprehensive) |
| **coverage-analysis** | Find and prioritize test gaps |
| **mutation-testing** | Verify test quality by introducing bugs |
| **property-based-testing** | Add algorithmic property tests |
| **test-gap-analysis** | Prioritize gaps by criticality |

### CLI Tools

9 deterministic utilities in `.claude/lib/atsr-tools/`:

| Tool | Purpose |
|------|---------|
| `atsr-size` | Assess codebase (LOC, test ratio, approach) |
| `atsr-detect` | Detect language, framework, test runner |
| `atsr-analyze-code` | Find functions needing tests (AST-based) |
| `atsr-generate-tests` | Create initial test templates |
| `atsr-coverage` | Run coverage analysis |
| `atsr-gaps` | Parse coverage, identify gaps |
| `atsr-mutate` | Run mutation testing |
| `atsr-survivors` | Analyze survived mutants |
| `atsr-recommend` | Generate test recommendations |

**Design Philosophy**: Skills teach WHEN and HOW. Tools do WHAT.

---

## ğŸ“Š Benchmarking

### What We Test

We compare **3 approaches** on the same codebases:

1. **Baseline**: Simple prompt asking to improve tests
2. **aTSR Skills**: Our complete skill system (`/refine-tests`)
3. **[Planned] obra TDD**: Using obra's test-driven-development skill

### Test Repositories

We use 3 real-world Python projects:

- **schedule** (~400 LOC, scheduling library)
- **mistune** (~2,600 LOC, Markdown parser)
- **click** (~3,500 LOC, CLI framework)

### How It Works

```bash
# 1. Set up test repos (downloads and prepares them)
bash src/setup_test_repos.sh examples

# 2. Create benchmark copies (4 configurations Ã— 3 repos = 12 runs)
for n in 1 2 3; do
  bash src/create_benchmark.sh examples/repos_reduced/ bench/bench$n/
  bash src/run_benchmark.sh bench/bench$n/
done

# 3. Evaluate results
bash src/evaluate_all.sh bench/ --output-dir evaluation_results

# 4. Visualize
python src/Create_visualization.py evaluation_results/summary.json results.png
```

**What gets measured:**
- Coverage % achieved
- Pass rate % (tests passing / total tests)
- Token usage (from Claude API logs)
- Tests added (count)
- Time to completion

### Current Limitations

âš ï¸ **Environment Isolation**: Benchmarks run **in series** using the **same Python environment**. Dependencies must be pre-installed globally. Future improvement: Use venvs per run.

âš ï¸ **obra Comparison**: Not yet implemented. Would require adding obra/superpowers skills to the test environment.

---

## ğŸ“ How It Works

### Scenario 1: No Tests Exist

```
User: /refine-tests

1. atsr-size â†’ detects 0 test files
2. test-creation skill activates:
   - atsr-analyze-code â†’ finds all 398 functions
   - atsr-generate-tests â†’ creates test templates
   - Human customizes TODOs (boundaries, errors, edge cases)
   - Tests run and pass
3. test-refinement skill takes over:
   - atsr-coverage â†’ measures baseline coverage
   - Adds missing edge cases
   - atsr-mutate â†’ verifies test quality

Result: 0 â†’ 85 tests, 78% coverage, 72% mutation score
```

### Scenario 2: Tests Exist, Need Improvement

```
User: /refine-tests

1. atsr-size â†’ detects tests exist, 45% coverage
2. test-refinement skill activates:
   - atsr-coverage â†’ identifies gaps
   - atsr-gaps â†’ prioritizes critical files (business logic first)
   - Human adds tests for gaps
   - atsr-mutate â†’ finds weak tests (boundary conditions, null checks)
   - atsr-survivors â†’ recommends killer tests
   - Human strengthens assertions

Result: 45% â†’ 82% coverage, mutation score 65% â†’ 78%
```

### Scenario 3: Tiny Codebase (< 100 LOC)

```
User: /refine-tests

1. atsr-size â†’ detects 73 LOC
2. test-suite-management skill:
   - Skips all tools (overhead not worth it)
   - Just reads code and tests directly
   - Identifies obvious gaps (boundaries, errors)
   - Writes them directly

Result: 5 minutes, perfect tests, zero tool overhead
```

---

## ğŸ“ Project Structure

```
aTSR/
â”œâ”€â”€ .claude/                        # Claude Code skills and tools
â”‚   â”œâ”€â”€ skills/                     # 7 modular skills (<500 words each)
â”‚   â”œâ”€â”€ commands/                   # Thin command wrappers
â”‚   â””â”€â”€ lib/atsr-tools/             # 9 CLI utilities
â”œâ”€â”€ src/                            # Benchmark infrastructure
â”‚   â”œâ”€â”€ create_benchmark.sh         # Set up benchmark runs
â”‚   â”œâ”€â”€ run_benchmark.sh            # Execute benchmarks
â”‚   â”œâ”€â”€ evaluate_all.sh             # Analyze results
â”‚   â”œâ”€â”€ setup_test_repos.sh         # Download test repositories
â”‚   â””â”€â”€ Create_visualization.py     # Generate plots
â”œâ”€â”€ initial_experiments/            # Original research experiments
â”‚   â”œâ”€â”€ monolithic_prompt.md        # Original 860-line prompt
â”‚   â””â”€â”€ prompt_variants/            # 6 prompt design iterations
â”œâ”€â”€ examples/                       # Generated test repos (gitignored)
â”œâ”€â”€ CLAUDE.md                       # Project memory and architecture
â””â”€â”€ README.md                       # This file
```

---

## ğŸ“š Documentation

- **[CLAUDE.md](CLAUDE.md)** - Complete architecture, competitive analysis, tool manifest, design decisions
- **[.claude/skills/](.claude/skills/)** - Individual skill documentation (WHEN to use, HOW to interpret)
- **[initial_experiments/](initial_experiments/)** - Original research: monolithic prompt evolution

---

## ğŸ”¬ Research

### Key Findings from Initial Experiments

Our initial experiment (in `initial_experiments/`) compared prompt designs:

- **Baseline** (simple prompt): **77.5% coverage**, 29.3M tokens
- **Monolithic** (860-line workflow): **74.7% coverage**, 35.8M tokens âŒ

**Lesson learned**: Simpler was better! Complex inline scripts caused agents to copy code wastefully.

### Skill-Based Redesign

Based on these findings, we built:
- âœ… Modular skills (< 500 words each) - no inline scripts
- âœ… CLI tools (deterministic, testable) - agents invoke, don't copy
- âœ… Clear decision trees (test-suite-management routes appropriately)
- âœ… Following obra/superpowers best practices

**Expected improvements:**
- Better coverage than baseline (systematic vs ad-hoc)
- Lower tokens than monolithic (no script copying)
- Complementary to obra (batch vs incremental)

---

## ğŸ¤ Contributing

We welcome contributions! Especially:

- Additional language support (Go, Rust, Ruby)
- New test quality metrics
- obra/superpowers integration for benchmarking
- Virtual environment isolation for benchmarks
- Skill improvements

---

## ğŸ“ˆ Roadmap

- [ ] Complete benchmark comparison (aTSR vs obra vs baseline)
- [ ] Add venv isolation to benchmarks
- [ ] Integration testing skill
- [ ] Performance testing patterns
- [ ] Security testing guidance
- [ ] Snapshot testing skill

---

## ğŸ“„ License

MIT License - see LICENSE file

---

## ğŸ™ Acknowledgments

- **obra/superpowers** - Inspiration for skill architecture and best practices
- **Claude Code team** - Skills system and documentation
- **CSE5095** - Course project framework

---

## ğŸ“ Contact

- **Issues**: https://github.com/rhowardstone/aTSR/issues
- **Progress Demo**: https://kaltura.uconn.edu/media/Kaltura+Capture+recording+-+October+6th+2025%2C+8%3A39%3A54+am/1_565tve3c

---

**aTSR**: Because test automation should be automatic.
